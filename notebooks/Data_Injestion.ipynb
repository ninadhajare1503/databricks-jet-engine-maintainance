{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e095dea1-6f25-45a1-b24e-a7beea56c5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Change to test dev pipeline1\n",
    "# 1. Create a widget to get the environment (dev or prod)\n",
    "dbutils.widgets.text(\"env\", \"dev\", \"Environment\")\n",
    "\n",
    "# 2. Get the value from the widget\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "\n",
    "# 3. Set your catalog and schema names based on the environment\n",
    "catalog_name = \"jet_engine_predictive_maintenance\"\n",
    "schema_name = f\"{env}\" # This will become \"dev\" or \"prod\"\n",
    "data_folder_path = \"./data\"\n",
    "\n",
    "print(f\"Running for environment: '{env}'\")\n",
    "print(f\"Target schema: '{catalog_name}.{schema_name}'\")\n",
    "\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS{catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS{schema_name}\")\n",
    "\n",
    "abs_data_folder_path = os.path.abspath(data_folder_path)\n",
    "\n",
    "os.listdir(abs_data_folder_path)\n",
    "csv_files = [f for f in all_files if f.endswith('.csv')]\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found in the specified directory.\")\n",
    "else:\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(abs_data_folder_path, csv_file)\n",
    "        table_name = os.path.splittext(csv_file)[0].replace('-','_')\n",
    "        full_table_path = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "# Read the CSV into a Spark DataFrame\n",
    "        # option(\"header\", \"true\"): Uses the first row as column headers\n",
    "        # option(\"inferSchema\", \"true\"): Automatically detects data types\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "                      .option(\"header\", \"true\") \\\n",
    "                      .option(\"inferSchema\", \"true\") \\\n",
    "                      .load(f\"file:{file_path}\") # 'file:' prefix is needed to read from local repo files\n",
    "\n",
    "        # Save the DataFrame as a table in Unity Catalog\n",
    "        # mode(\"overwrite\"): Replaces the table if it already exists\n",
    "        df.write.mode(\"overwrite\").saveAsTable(full_table_path)\n",
    "\n",
    "        print(f\"âœ… Successfully created table '{table_name}'.\")\n",
    "\n",
    "print(\"\\n--- Ingestion complete! ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Injestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
